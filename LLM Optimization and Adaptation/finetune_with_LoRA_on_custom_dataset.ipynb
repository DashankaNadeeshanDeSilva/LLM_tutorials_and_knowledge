{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLM uing LoRA with Custom Dataset\n",
    "\n",
    "Intro: This notebook works on fine-tuning a LLM on a custom dataset using Low Rank Adaptartion (LoRA) technique.\n",
    "\n",
    "Method: LoRA is a fine-tuning techniqe that introduces trainable low-rank matrices (way smaller than original weight matrix) without training all the paramaters of the model. These low rank matrices are trained on the dataset (e.g. on domain specifi task) while original LLM model parameters are frozen, and then added to the model to introduce task specific specialization to the LLM\n",
    "\n",
    "Dataset: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pretrained Llama 3.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the model name (replace with the actual LLaMA checkpoint)\n",
    "model_name = \"llama-3.2\"  # Or path to the local LLaMA checkpoint\n",
    "\n",
    "# Load the pre-trained LLaMA model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # Rank of the LoRA update matrices\n",
    "    lora_alpha=32,      # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
    "    lora_dropout=0.1,   # Dropout probability\n",
    "    bias=\"none\",        # LoRA bias type\n",
    "    task_type=\"CAUSAL_LM\"  # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Find Target Modules in the Model like query and key projetcion layers from the attention mechanism of the transformer based Llama model\n",
    "- Print the moodel architecture `print(model)`\n",
    "- This displays all layers and their names. Look for components related to attention (e.g., `self_attn, q_proj, k_proj,` etc.).\n",
    "- Search for Layer Names: Manually search for names like `q_proj, v_proj`, or other layers based on the task.\n",
    "- Common Attention Layers in Transformers:\n",
    "    - Query (q_proj): Projects input into queries for self-attention.\n",
    "    - Key (k_proj): Projects input into keys for self-attention.\n",
    "    - Value (v_proj): Projects input into values for self-attention.\n",
    "- Example output:\n",
    "```\n",
    "(transformer.layers.0.self_attn.q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "(transformer.layers.0.self_attn.k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "(transformer.layers.0.self_attn.v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "```\n",
    "- From the output: `q_proj` and `v_proj` are layers in the `self_attn` module of the first Transformer layer (`transformer.layers.0`).\n",
    "- LoRA can be applied to all relevant layers throughout the model, depending on the specified configuration\n",
    "- When target_modules=[\"q_proj\", \"v_proj\"] defined in the LoRA configuration, LoRA will automatically locate and modify the q_proj and v_proj layers in all transformer layers (not just the first one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "letter_gen_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
