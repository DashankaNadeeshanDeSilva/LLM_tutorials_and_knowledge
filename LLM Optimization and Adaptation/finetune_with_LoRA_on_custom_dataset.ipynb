{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLM uing LoRA with Custom Dataset\n",
    "\n",
    "Intro: This notebook works on fine-tuning a LLM on a custom dataset using Low Rank Adaptartion (LoRA) technique.\n",
    "\n",
    "Method: LoRA is a fine-tuning techniqe that introduces trainable low-rank matrices (way smaller than original weight matrix) without training all the paramaters of the model. These low rank matrices are trained on the dataset (e.g. on domain specifi task) while original LLM model parameters are frozen, and then added to the model to introduce task specific specialization to the LLM\n",
    "\n",
    "Dataset: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pretrained Llama 3.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the model name (replace with the actual LLaMA checkpoint)\n",
    "model_name = \"llama-3.2\"  # Or path to the local LLaMA checkpoint\n",
    "\n",
    "# Load the pre-trained LLaMA model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # Rank of the LoRA update matrices\n",
    "    lora_alpha=32,      # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
    "    lora_dropout=0.1,   # Dropout probability\n",
    "    bias=\"none\",        # LoRA bias type\n",
    "    task_type=\"CAUSAL_LM\"  # Task type for causal language modeling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the Base Model and Enabling LoRA for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "# the base model is automatically frozen when LoRA is integrated using the PEFT library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Find Target Modules in the Model like query and key projetcion layers from the attention mechanism of the transformer based Llama model\n",
    "- Print the moodel architecture `print(model)`\n",
    "- This displays all layers and their names. Look for components related to attention (e.g., `self_attn, q_proj, k_proj,` etc.).\n",
    "- Search for Layer Names: Manually search for names like `q_proj, v_proj`, or other layers based on the task.\n",
    "- Common Attention Layers in Transformers:\n",
    "    - Query (q_proj): Projects input into queries for self-attention.\n",
    "    - Key (k_proj): Projects input into keys for self-attention.\n",
    "    - Value (v_proj): Projects input into values for self-attention.\n",
    "- Example output:\n",
    "```\n",
    "(transformer.layers.0.self_attn.q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "(transformer.layers.0.self_attn.k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "(transformer.layers.0.self_attn.v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "```\n",
    "- From the output: `q_proj` and `v_proj` are layers in the `self_attn` module of the first Transformer layer (`transformer.layers.0`).\n",
    "- LoRA can be applied to all relevant layers throughout the model, depending on the specified configuration\n",
    "- When target_modules=[\"q_proj\", \"v_proj\"] defined in the LoRA configuration, LoRA will automatically locate and modify the q_proj and v_proj layers in all transformer layers (not just the first one).\n",
    "- The PEFT library will:\n",
    "    - Identify all instances of q_proj and v_proj across the model.\n",
    "    - Insert the LoRA updates (low-rank matrices) into each of these layers.\n",
    "\n",
    "Why Apply LoRA to All Layers : Distributed Adaptation: Applying LoRA to all transformer layers allows the model to adapt globally to the new task, and each layer contributes differently to the overall learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model to confirm which parameters are trainable\n",
    "print(model)\n",
    "\n",
    "# Optional: Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps:\n",
    "\n",
    "- Dataset preparation and tokenization.\n",
    "- Writing a training loop with LoRA integration.\n",
    "- Evaluation and saving the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent: Thank you for calling BrownBox Customer Support. My name is Sarah. How can I assist you today? Customer: Hi Sarah, my name is Jane and I have a question about my order. Agent: Hi Jane, I'm sorry to hear that. Could you please provide me with your order number so I can look into it? Customer: Sure, my order number is BB987654321. Agent: Thank you for providing that information, Jane. I see that your order was placed on June 1st. How can I assist you with it? Customer: I'm really frustrated because my dishwasher was out for delivery yesterday, but I still haven't received it. I was wondering if you can help me track it down. Agent: I'm sorry to hear that, Jane. Let me check the status of your order for you. Please hold on for a moment while I access the information. (Customer is put on hold for a minute) Agent: Thank you for waiting, Jane. I have checked the status of your order. It appears that your package has been shipped, and the tracking number is BB123456789. You can track your package using that number on our website or the courier's website. Customer: Yes, I have been tracking it. It says it was out for delivery yesterday, but I still haven't received it yet. Agent: I understand your frustration, Jane. Let me investigate the situation further to provide you with more information. May I place you on hold for a moment? (Customer is put on hold for a few minutes) Agent: Thank you for waiting, Jane. I apologize for the inconvenience caused. It seems that your package experienced a delay during transit. I have escalated the issue to our shipping department, and they will contact the courier to expedite the delivery. Please accept our apologies for the delay. Customer: I'm not happy about this at all, Sarah. I paid for express shipping, and I expect to receive my package on time. Agent: I completely understand, Jane. We take delivery times very seriously, and we are doing everything we can to expedite the delivery. I will personally follow up with the shipping department to ensure that your package is delivered as soon as possible. Is there anything else I can assist you with? Customer: No, that's all for now. I just want my dishwasher delivered as soon as possible. Agent: I completely understand, Jane. We will do our best to make sure that happens. Thank you for contacting BrownBox Customer Support. If you have any further questions or concerns, don't hesitate to reach out to us. Customer: Alright, thank you, Sarah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "letter_gen_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
