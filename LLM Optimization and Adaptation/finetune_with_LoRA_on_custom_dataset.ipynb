{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLM uing LoRA with Custom Dataset\n",
    "\n",
    "Intro: This notebook works on fine-tuning a LLM on a custom dataset using Low Rank Adaptartion (LoRA) technique.\n",
    "\n",
    "Method: LoRA is a fine-tuning techniqe that introduces trainable low-rank matrices (way smaller than original weight matrix) without training all the paramaters of the model. These low rank matrices are trained on the dataset (e.g. on domain specifi task) while original LLM model parameters are frozen, and then added to the model to introduce task specific specialization to the LLM\n",
    "\n",
    "Dataset: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "Let me know if you want to dive into:\n",
    "\n",
    "\n",
    "I would like to train Llama 3.2 model on custom dataset using LoRA technique. Guide and teach me on these points.\n",
    "1. Detailed LoRA configuration and its parameters.\n",
    "2. Code walkthrough for integrating LoRA with LLaMA.\n",
    "3. Custom Dataset preparation and task-specific tuning.\n",
    "4. How to train the model with LoRA on this dataset.\n",
    "\n",
    "If this is too big, start with Overview and then we can dive into details. later on.\n",
    "\n",
    "\n",
    "Later onDeployment of a fine-tuned LLaMA with LoRA. Like serving with Ollama and publish in Hugging faces\n",
    "\n",
    "Now I would like you to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Loading the pretrained Llama 3.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the model name (replace with the actual LLaMA checkpoint)\n",
    "model_name = \"llama-3.2\"  # Or path to the local LLaMA checkpoint\n",
    "\n",
    "# Load the pre-trained LLaMA model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example usage of tokenizer (not mandatory here but useful for later steps)\n",
    "text = \"This is an example sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
