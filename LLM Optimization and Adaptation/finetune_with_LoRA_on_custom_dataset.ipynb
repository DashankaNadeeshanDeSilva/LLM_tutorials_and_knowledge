{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLM uing LoRA with Custom Dataset\n",
    "\n",
    "Intro: This notebook works on fine-tuning a LLM on a custom dataset using Low Rank Adaptartion (LoRA) technique.\n",
    "\n",
    "Method: LoRA is a fine-tuning techniqe that introduces trainable low-rank matrices (way smaller than original weight matrix) without training all the paramaters of the model. These low rank matrices are trained on the dataset (e.g. on domain specifi task) while original LLM model parameters are frozen, and then added to the model to introduce task specific specialization to the LLM\n",
    "\n",
    "Dataset: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pretrained Llama 3.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the model name (replace with the actual LLaMA checkpoint)\n",
    "model_name = \"llama-3.2\"  # Or path to the local LLaMA checkpoint\n",
    "\n",
    "# Load the pre-trained LLaMA model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # Rank of the LoRA update matrices\n",
    "    lora_alpha=32,      # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
    "    lora_dropout=0.1,   # Dropout probability\n",
    "    bias=\"none\",        # LoRA bias type\n",
    "    task_type=\"CAUSAL_LM\"  # Task type for causal language modeling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the Base Model and Enabling LoRA for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "# the base model is automatically frozen when LoRA is integrated using the PEFT library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Find Target Modules in the Model like query and key projetcion layers from the attention mechanism of the transformer based Llama model\n",
    "- Print the moodel architecture `print(model)`\n",
    "- This displays all layers and their names. Look for components related to attention (e.g., `self_attn, q_proj, k_proj,` etc.).\n",
    "- Search for Layer Names: Manually search for names like `q_proj, v_proj`, or other layers based on the task.\n",
    "- Common Attention Layers in Transformers:\n",
    "    - Query (q_proj): Projects input into queries for self-attention.\n",
    "    - Key (k_proj): Projects input into keys for self-attention.\n",
    "    - Value (v_proj): Projects input into values for self-attention.\n",
    "- Example output:\n",
    "```\n",
    "(transformer.layers.0.self_attn.q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "(transformer.layers.0.self_attn.k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "(transformer.layers.0.self_attn.v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "```\n",
    "- From the output: `q_proj` and `v_proj` are layers in the `self_attn` module of the first Transformer layer (`transformer.layers.0`).\n",
    "- LoRA can be applied to all relevant layers throughout the model, depending on the specified configuration\n",
    "- When target_modules=[\"q_proj\", \"v_proj\"] defined in the LoRA configuration, LoRA will automatically locate and modify the q_proj and v_proj layers in all transformer layers (not just the first one).\n",
    "- The PEFT library will:\n",
    "    - Identify all instances of q_proj and v_proj across the model.\n",
    "    - Insert the LoRA updates (low-rank matrices) into each of these layers.\n",
    "\n",
    "Why Apply LoRA to All Layers : Distributed Adaptation: Applying LoRA to all transformer layers allows the model to adapt globally to the new task, and each layer contributes differently to the overall learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model to confirm which parameters are trainable\n",
    "print(model)\n",
    "\n",
    "# Optional: Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps:\n",
    "\n",
    "- Dataset preparation and tokenization.\n",
    "- Writing a training loop with LoRA integration.\n",
    "- Evaluation and saving the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "letter_gen_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
