
Word embeddings

Definition: In the NLP domain, converting words to or representing words in real-valued vectors (often multi-dimensional) is commonly referred to as word embeddings. This basically gives the ability to computers to understand and learn about text-based content in a better and productive way. There exist many word embeddings methods and techniques, which can be mainly categorize into two types: statistical and contextual word embeddings.

### Tranditonal Embedding Techniques

- One-hor-encoding
- Bag of words
- TF-IDF (Term Frequency-Inverse Document Frequency)
- Word2Vec
- Glove (Global Vectors for Word Representation)
- FastText

### Contextual Embedding Techniques