
Word embeddings

Defintion: In NLP domain, converting words to or represent words in real-valued vecors (often multi-dimesional) is commenly referred to as word embeddings. This basically gives the ability to computers to understand and learn about text-based content in a better and productive way. There exisit many word embeddings methods and techniques, which can be mainly categorize into two types: statistical and contextual word embeddings.